{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etcx1hbHyRYf"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis\n",
        "!pip install pandas==1.5.3\n",
        "# must restart runtime to use installed libraries"
      ],
      "metadata": {
        "id": "wmBV4DXpKOAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5aXq8JxqWbx"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis.gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "\n",
        "import gensim\n",
        "from gensim import models\n",
        "from gensim import corpora\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from collections import defaultdict\n",
        "\n",
        "import warnings\n",
        "import pprint\n",
        "import os\n",
        "import zipfile\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEJ4sCKTyvRK"
      },
      "source": [
        "Download files, set options, and initalize libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS9lqDNUWwFc"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('vader_lexicon')\n",
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "pd.set_option('display.max_rows', None)\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjKWswtNytZ3"
      },
      "source": [
        "Download and initialize glove dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl1TIWQV39BB"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate \\http://nlp.stanford.edu/data/glove.6B.zip \\-O /tmp/glove.6B.zip\n",
        "with zipfile.ZipFile('/tmp/glove.6B.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/tmp/glove')\n",
        "\n",
        "# create dictionary for glove\n",
        "embeddings_index = {}\n",
        "temp = open('/tmp/glove/glove.6B.100d.txt')\n",
        "for line in temp:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "temp.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uePXzPJjzRGj"
      },
      "source": [
        "Function to remove nonenglish and meaningless words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P5AZvoV5yk2"
      },
      "outputs": [],
      "source": [
        "meaningless_words = [\n",
        "    \"theyre\", \"unable\", \"today\", \"caused\", \"right\", \"line\", \"enter\", \"hold\", \"taking\",\n",
        "    \"lisa\", \"ines\", \"susan\", \"mike\", \"many\", \"placed\", \"many\", \"marty\", \"ines\", \"ryan\",\n",
        "    \"cassy\", \"joseph\", \"matthew\", \"lissa\", \"nicole\", \"annette\", \"rachel\", \"rosie\", \"make\",\n",
        "    \"phil\", \"mark\", \"viviana\", \"going\", \"came\", \"acct\", \"sure\", \"hope\", \"youre\", \"dont\",\n",
        "    \"cant\", \"much\", \"something\", \"thank\", \"thanks\", \"please\", \"let\", \"may\", \"via\", \"back\",\n",
        "    \"would\", \"need\", \"like\", \"using\", \"day\", \"know\", \"heard\", \"take\", \"see\", \"made\",\n",
        "    \"someone\", \"wed\", \"welcome\", \"tell\", \"what\", \"whats\", \"feel\", \"type\", \"havent\",\n",
        "    \"want\", \"get\", \"reach\", \"within\", \"with\", \"hear\", \"etc\", \"im\", \"one\", \"saw\",\n",
        "    \"hello\", \"wanted\", \"also\", \"hey\", \"reaching\", \"look\", \"could\", \"full\", \"youd\",\n",
        "    \"could\", \"looking\", \"way\", \"able\", \"lookout\", \"try\", \"show\", \"bit\", \"yes\", \"isnt\",\n",
        "    \"tried\", \"gotcha\", \"didnt\", \"might\"\n",
        "]\n",
        "\n",
        "def remove_nonenglish_meaningless(word_list):\n",
        "    cleaned_word_list = []\n",
        "    for word in word_list:\n",
        "        # word cannot be in meaningless array\n",
        "        if word not in meaningless_words:\n",
        "            try:\n",
        "                # append word if word exists in dictionary\n",
        "                embeddings_index[word]\n",
        "                cleaned_word_list.append(word)\n",
        "            except:\n",
        "                pass\n",
        "    return cleaned_word_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx-_iXgAzolA"
      },
      "source": [
        "Process the data and create dataframes\n",
        "- Lowercase everything\n",
        "- Remove symbols\n",
        "- Remove extra whitespace\n",
        "- Tokenize tweets\n",
        "- Remove stopwords\n",
        "- Lemmatize tweets\n",
        "- Remove nonenglish words\n",
        "- Remove meaningless words\n",
        "- Drop unnecessary columns\n",
        "- Set minimum length for tweets\n",
        "- Remove empty values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSP6KtPArwTT"
      },
      "outputs": [],
      "source": [
        "def process_data(url):\n",
        "    csv = pd.read_csv(url)\n",
        "    df = pd.DataFrame(csv)\n",
        "\n",
        "    df.replace(\"\", np.nan, inplace=True)\n",
        "    df.dropna(inplace=True)\n",
        "    df = df[df[\"Tweet\"].str.len() >= 80]\n",
        "    df = df.reset_index(drop=True)\n",
        "    df = df.drop(['Tweet ID', 'URL', 'Username', 'User Bio'], axis=1)\n",
        "\n",
        "    df[\"tweet-cleaned\"] = df[\"Tweet\"].str.lower().str.replace('[^A-Za-z ]','').str.replace(' +',' ').str.strip().str.replace('[^a-zA-Z\\s]', '', regex=True)\n",
        "    df[\"tweet-cleaned\"] = df[\"tweet-cleaned\"].fillna('').apply(lambda word: word_tokenize(word))\n",
        "    df[\"tweet-cleaned\"] = df[\"tweet-cleaned\"].apply(lambda word_list: [word for word in word_list if word not in stopwords])\n",
        "\n",
        "    df[\"tweet-cleaned-lemma\"] = df[\"tweet-cleaned\"].apply(lambda word_list: [lemmatizer.lemmatize(word) for word in word_list])\n",
        "    df[\"tweet-cleaned-lemma\"] = df[\"tweet-cleaned-lemma\"].apply(lambda word_list: remove_nonenglish_meaningless(word_list))\n",
        "    return df\n",
        "\n",
        "americanexpress_df = process_data(\"https://raw.githubusercontent.com/adb8/Bank-Data/main/americanexpress_data.csv\")\n",
        "bankofamerica_df = process_data(\"https://raw.githubusercontent.com/adb8/Bank-Data/main/bankofamerica_data.csv\")\n",
        "wellsfargo_df = process_data(\"https://raw.githubusercontent.com/adb8/Bank-Data/main/wellsfargo_data.csv\")\n",
        "capitalone_df = process_data(\"https://raw.githubusercontent.com/adb8/Bank-Data/main/capitalone_data.csv\")\n",
        "chase_df = process_data(\"https://raw.githubusercontent.com/adb8/Bank-Data/main/chase_data.csv\")\n",
        "citi_df = process_data(\"https://raw.githubusercontent.com/adb8/Bank-Data/main/citi_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv2kgsTszwqb"
      },
      "source": [
        "Function to turn dataframe to array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EB6miBO8bM6v"
      },
      "outputs": [],
      "source": [
        "def df_to_array(df):\n",
        "    document_arr = []\n",
        "    for index, row in df.iterrows():\n",
        "        row = row[\"tweet-cleaned-lemma\"]\n",
        "        # rows cannot be blank\n",
        "        if len(row) == 0:\n",
        "            continue\n",
        "        document = []\n",
        "        for word in row:\n",
        "            # words must be at least 4 chars\n",
        "            if len(word) < 4:\n",
        "                continue\n",
        "            document.append(word)\n",
        "        if len(document) > 0:\n",
        "            document_arr.append(document)\n",
        "    return document_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIVHzp4h0EVk"
      },
      "source": [
        "Function to perform topic modelling when number of topics is unknown\n",
        "(finds best number of topics using coherence values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBeYVlUBbDMf"
      },
      "outputs": [],
      "source": [
        "def topic_modelling_num_topics_unknown(df, testing_limit):\n",
        "    FREQUENCY_REQUIREMENT = 1\n",
        "    document_arr = df_to_array(df)\n",
        "\n",
        "    # counts frequency of each word\n",
        "    frequency = defaultdict(int)\n",
        "    for document in document_arr:\n",
        "        for word in document:\n",
        "             frequency[word] += 1\n",
        "\n",
        "    # creates corpus of words with enough frequency\n",
        "    processed_corpus = [[word for word in document if frequency[word] > FREQUENCY_REQUIREMENT] for document in document_arr]\n",
        "    dictionary = corpora.Dictionary(processed_corpus)\n",
        "    bow_corpus = [dictionary.doc2bow(document) for document in processed_corpus]\n",
        "\n",
        "    # creates ldamodels with variable number of topics and records coherence values\n",
        "    def simulate_coherence_values(dictionary, corpus, texts, testing_limit):\n",
        "        coherence_values_arr = []\n",
        "        num_topics_arr = []\n",
        "        for num_topics in range(2, testing_limit):\n",
        "            lda_model = models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=np.random.RandomState(100))\n",
        "            coherencemodel = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "            coherence_values_arr.append(coherencemodel.get_coherence())\n",
        "            num_topics_arr.append(num_topics)\n",
        "        return coherence_values_arr, num_topics_arr\n",
        "\n",
        "    coherence_values_arr, num_topics_arr = simulate_coherence_values(dictionary=dictionary, corpus=bow_corpus, texts=df[\"tweet-cleaned-lemma\"], testing_limit=testing_limit)\n",
        "\n",
        "    # finds and returns model with highest coherence value\n",
        "    best_num_topics = 0\n",
        "    highest_cv_value = 0\n",
        "    for i in range(len(coherence_values_arr)):\n",
        "        if coherence_values_arr[i] > highest_cv_value:\n",
        "            highest_cv_value = coherence_values_arr[i]\n",
        "            best_num_topics = num_topics_arr[i]\n",
        "\n",
        "    lda_model = models.LdaMulticore(corpus=bow_corpus, id2word=dictionary, num_topics=best_num_topics, random_state=np.random.RandomState(100))\n",
        "    return lda_model, best_num_topics, bow_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maTn1Tpk0YEy"
      },
      "source": [
        "Function to perform topic modelling when number of topics is known"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_VqT6Kna9xe"
      },
      "outputs": [],
      "source": [
        "def topic_modelling_num_topics_manual(df, num_topics):\n",
        "    FREQUENCY_REQUIREMENT = 1\n",
        "    document_arr = df_to_array(df)\n",
        "\n",
        "    # counts frequency of each word\n",
        "    frequency = defaultdict(int)\n",
        "    for document in document_arr:\n",
        "        for word in document:\n",
        "             frequency[word] += 1\n",
        "\n",
        "    # creates corpus of words with enough frequency\n",
        "    processed_corpus = [[word for word in document if frequency[word] > FREQUENCY_REQUIREMENT] for document in document_arr]\n",
        "    dictionary = corpora.Dictionary(processed_corpus)\n",
        "    bow_corpus = [dictionary.doc2bow(document) for document in processed_corpus]\n",
        "\n",
        "    # returns model with chosen number of topics\n",
        "    lda_model = models.LdaMulticore(corpus=bow_corpus, id2word=dictionary, num_topics=num_topics, random_state=np.random.RandomState(100))\n",
        "    return lda_model, num_topics, bow_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Joq0CoD0d6K"
      },
      "source": [
        "Performs topic modelling on each bank given ideal number of topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30twG3IHda5p"
      },
      "outputs": [],
      "source": [
        "americanexpress_model, americanexpress_num_topics, americanexpress_corpus = topic_modelling_num_topics_manual(df=americanexpress_df, num_topics=4)\n",
        "bankofamerica_model, bankofamerica_num_topics, bankofamerica_corpus = topic_modelling_num_topics_manual(df=bankofamerica_df, num_topics=4)\n",
        "wellsfargo_model, wellsfargo_num_topics, wellsfargo_corpus = topic_modelling_num_topics_manual(df=wellsfargo_df, num_topics=3)\n",
        "capitalone_model, capitalone_num_topics, capitalone_corpus = topic_modelling_num_topics_manual(df=capitalone_df, num_topics=7)\n",
        "chase_model, chase_num_topics, chase_corpus = topic_modelling_num_topics_manual(df=chase_df, num_topics=8)\n",
        "citi_model, citi_num_topics, citi_corpus = topic_modelling_num_topics_manual(df=citi_df, num_topics=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LbzN4yp1F2c"
      },
      "source": [
        "Prints the first 30 words of each topic from each bank\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sLoYVJJHvR8"
      },
      "outputs": [],
      "source": [
        "NUMBER_WORDS = 30\n",
        "print(\"American Express topics: \" + str(americanexpress_num_topics))\n",
        "pprint.pprint(americanexpress_model.print_topics(num_words=NUMBER_WORDS))\n",
        "\n",
        "print(\"Bank of America topics: \" + str(bankofamerica_num_topics))\n",
        "pprint.pprint(bankofamerica_model.print_topics(num_words=NUMBER_WORDS))\n",
        "\n",
        "print(\"Wells Fargo topics: \" + str(wellsfargo_num_topics))\n",
        "pprint.pprint(wellsfargo_model.print_topics(num_words=NUMBER_WORDS))\n",
        "\n",
        "print(\"Capital One topics: \" + str(capitalone_num_topics))\n",
        "pprint.pprint(capitalone_model.print_topics(num_words=NUMBER_WORDS))\n",
        "\n",
        "print(\"Chase topics: \" + str(chase_num_topics))\n",
        "pprint.pprint(chase_model.print_topics(num_words=NUMBER_WORDS))\n",
        "\n",
        "print(\"Citi topics: \" + str(citi_num_topics))\n",
        "pprint.pprint(citi_model.print_topics(num_words=NUMBER_WORDS))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6s2W8DI1Jqk"
      },
      "source": [
        "Creates and saves pyLDAvis visualizations of each banks' topics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cmjfhv4bEko"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "americanexpress_vis = pyLDAvis.gensim.prepare(americanexpress_model, americanexpress_corpus, dictionary=americanexpress_model.id2word)\n",
        "bankofamerica_vis = pyLDAvis.gensim.prepare(bankofamerica_model, bankofamerica_corpus, dictionary=bankofamerica_model.id2word)\n",
        "wellsfargo_vis = pyLDAvis.gensim.prepare(wellsfargo_model, wellsfargo_corpus, dictionary=wellsfargo_model.id2word)\n",
        "capitalone_vis = pyLDAvis.gensim.prepare(capitalone_model, capitalone_corpus, dictionary=capitalone_model.id2word)\n",
        "chase_vis = pyLDAvis.gensim.prepare(chase_model, chase_corpus, dictionary=chase_model.id2word)\n",
        "citi_vis = pyLDAvis.gensim.prepare(citi_model, citi_corpus, dictionary=citi_model.id2word)\n",
        "\n",
        "pyLDAvis.save_html(americanexpress_vis, 'americanexpress.html')\n",
        "pyLDAvis.save_html(bankofamerica_vis, 'bankofamerica.html')\n",
        "pyLDAvis.save_html(wellsfargo_vis, 'wellsfargo.html')\n",
        "pyLDAvis.save_html(capitalone_vis, 'capitalone.html')\n",
        "pyLDAvis.save_html(chase_vis, 'chase.html')\n",
        "pyLDAvis.save_html(citi_vis, 'citi.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwkru5Jl1SU-"
      },
      "source": [
        "Divides the Citi dataframe into years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUpz-_aYepUP"
      },
      "outputs": [],
      "source": [
        "def count_tweets_from_year(df, years):\n",
        "    # counts tweets from bank for given year\n",
        "    times = 0\n",
        "    for index, row in df.iterrows():\n",
        "        if years in row[\"Timestamp\"]:\n",
        "            times += 1\n",
        "    return times\n",
        "\n",
        "num_times_2023 = count_tweets_from_year(citi_df, '2023')\n",
        "num_times_2022 = num_times_2023 + count_tweets_from_year(citi_df, '2022')\n",
        "num_times_2021 = num_times_2022 + count_tweets_from_year(citi_df, '2021')\n",
        "num_times_2020 = num_times_2021 + count_tweets_from_year(citi_df, '2020')\n",
        "num_times_2019 = num_times_2020 + count_tweets_from_year(citi_df, '2019')\n",
        "num_times_2018 = num_times_2019 + count_tweets_from_year(citi_df, '2018')\n",
        "\n",
        "citi_2023_df = citi_df.head(num_times_2023)\n",
        "citi_2022_df = citi_df.iloc[num_times_2023:num_times_2022]\n",
        "citi_2021_df = citi_df.iloc[num_times_2022:num_times_2021]\n",
        "citi_2020_df = citi_df.iloc[num_times_2021:num_times_2020]\n",
        "citi_2019_df = citi_df.iloc[num_times_2020:num_times_2019]\n",
        "citi_2018_df = citi_df.iloc[num_times_2019:num_times_2018]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgG8ujgq1ZeC"
      },
      "source": [
        "Performs topic modelling on each year given ideal number of topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGoDsQPRer8k"
      },
      "outputs": [],
      "source": [
        "citi_2023_model, citi_2023_num_topics, citi_2023_corpus = topic_modelling_num_topics_manual(df=citi_2023_df, num_topics=3)\n",
        "citi_2022_model, citi_2022_num_topics, citi_2022_corpus = topic_modelling_num_topics_manual(df=citi_2022_df, num_topics=4)\n",
        "citi_2021_model, citi_2021_num_topics, citi_2021_corpus = topic_modelling_num_topics_manual(df=citi_2021_df, num_topics=4)\n",
        "citi_2020_model, citi_2020_num_topics, citi_2020_corpus = topic_modelling_num_topics_manual(df=citi_2020_df, num_topics=4)\n",
        "citi_2019_model, citi_2019_num_topics, citi_2019_corpus = topic_modelling_num_topics_manual(df=citi_2019_df, num_topics=3)\n",
        "citi_2018_model, citi_2018_num_topics, citi_2018_corpus = topic_modelling_num_topics_manual(df=citi_2018_df, num_topics=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9nopX4q1hbi"
      },
      "source": [
        "Prints the first 30 words of each topic from each year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJSXI88QsCTT"
      },
      "outputs": [],
      "source": [
        "NUMBER_WORDS = 30\n",
        "print(\"Year_2023\" + str(citi_2023_num_topics))\n",
        "pprint.pprint(citi_2023_model.print_topics(num_words=NUMBER_WORDS))\n",
        "\n",
        "print(\"Year_2022\" + str(citi_2022_num_topics))\n",
        "pprint.pprint(citi_2022_model.print_topics(num_words=NUMBER_WORDS))\n",
        "\n",
        "print(\"Year_2023\" + str(citi_2021_num_topics))\n",
        "pprint.pprint(citi_2021_model.print_topics(num_words=NUMBER_WORDS))\n",
        "\n",
        "print(\"Year_2020\" + str(citi_2020_num_topics))\n",
        "pprint.pprint(citi_2020_model.print_topics(num_words=NUMBER_WORDS))\n",
        "\n",
        "print(\"Year_2019\" + str(citi_2019_num_topics))\n",
        "pprint.pprint(citi_2019_model.print_topics(num_words=NUMBER_WORDS))\n",
        "\n",
        "print(\"Year_2018\" + str(citi_2018_num_topics))\n",
        "pprint.pprint(citi_2019_model.print_topics(num_words=NUMBER_WORDS))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipq5wNXx19Gp"
      },
      "source": [
        "Creates and saves pyLDAvis visualizations of each years' topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9hew-brsebb"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "citi_2023_vis = pyLDAvis.gensim.prepare(citi_2023_model, citi_2023_corpus, dictionary=citi_2023_model.id2word)\n",
        "citi_2022_vis = pyLDAvis.gensim.prepare(citi_2022_model, citi_2022_corpus, dictionary=citi_2022_model.id2word)\n",
        "citi_2021_vis = pyLDAvis.gensim.prepare(citi_2021_model, citi_2021_corpus, dictionary=citi_2021_model.id2word)\n",
        "citi_2020_vis = pyLDAvis.gensim.prepare(citi_2020_model, citi_2020_corpus, dictionary=citi_2020_model.id2word)\n",
        "citi_2019_vis = pyLDAvis.gensim.prepare(citi_2019_model, citi_2019_corpus, dictionary=citi_2019_model.id2word)\n",
        "citi_2018_vis = pyLDAvis.gensim.prepare(citi_2018_model, citi_2018_corpus, dictionary=citi_2018_model.id2word)\n",
        "\n",
        "pyLDAvis.save_html(citi_2023_vis, '2023.html')\n",
        "pyLDAvis.save_html(citi_2022_vis, '2022.html')\n",
        "pyLDAvis.save_html(citi_2021_vis, '2021.html')\n",
        "pyLDAvis.save_html(citi_2020_vis, '2020.html')\n",
        "pyLDAvis.save_html(citi_2019_vis, '2019.html')\n",
        "pyLDAvis.save_html(citi_2018_vis, '2018.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb91JOAy2J_o"
      },
      "source": [
        "Performs sentiment analysis using TextBlob for each bank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soTFV7r9i7O3"
      },
      "outputs": [],
      "source": [
        "def analyze_sentiment_textblob(df):\n",
        "    def analyze_sentiment_textblob(text):\n",
        "        analysis = TextBlob(text)\n",
        "        if analysis.sentiment.polarity > 0:\n",
        "            return \"Positive\"\n",
        "        elif analysis.sentiment.polarity == 0:\n",
        "            return \"Neutral\"\n",
        "        else:\n",
        "            return \"Negative\"\n",
        "    df['tweet-sentiment-textblob'] = df[\"Tweet\"].apply(analyze_sentiment_textblob)\n",
        "    return df\n",
        "\n",
        "americanexpress_df = analyze_sentiment_textblob(americanexpress_df)\n",
        "bankofamerica_df = analyze_sentiment_textblob(bankofamerica_df)\n",
        "wellsfargo_df = analyze_sentiment_textblob(wellsfargo_df)\n",
        "capitalone_df = analyze_sentiment_textblob(capitalone_df)\n",
        "chase_df = analyze_sentiment_textblob(chase_df)\n",
        "citi_df = analyze_sentiment_textblob(citi_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHFwOiiTpAye"
      },
      "source": [
        "Creates a bar chart and pie chart of TextBlob sentiment analysis results for each bank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U10xtWuho8L8"
      },
      "outputs": [],
      "source": [
        "bank_dfs = {\n",
        "    \"Citi\": citi_df,\n",
        "    \"American Express\": americanexpress_df,\n",
        "    \"Bank of America\": bankofamerica_df,\n",
        "    \"Wells Fargo\": wellsfargo_df,\n",
        "    \"Capital One\": capitalone_df,\n",
        "    \"Chase\": chase_df\n",
        "}\n",
        "\n",
        "for bank_name, df in bank_dfs.items():\n",
        "    fig, subPlot = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
        "    GroupedData = df.groupby('tweet-sentiment-textblob').size()\n",
        "\n",
        "    # convert frequency to percentage\n",
        "    total_tweets = len(df)\n",
        "    percentages = (GroupedData / total_tweets) * 100\n",
        "\n",
        "    bar_chart = percentages.plot(kind='bar', ax=subPlot[0], color=['dodgerblue', 'royalblue', 'deepskyblue'])\n",
        "    bar_chart.set_xlabel(\"Sentiment Score\")\n",
        "    bar_chart.set_ylabel(\"Percentage of Tweets\")\n",
        "    bar_chart.set_title(\"Sentiment Score vs. Tweet Percentage for \" + bank_name)\n",
        "    bar_chart.set_ylim(0, 100)\n",
        "\n",
        "    pie_chart = GroupedData.plot(kind='pie', ax=subPlot[1], colors=['dodgerblue', 'royalblue', 'deepskyblue'])\n",
        "    pie_chart.set_title(\"Sentiment Score for \" + bank_name)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP0vDuui2gLl"
      },
      "source": [
        "Performs sentiment analysis using Vader for each bank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y725bkTySJ1W"
      },
      "outputs": [],
      "source": [
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def analyze_sentiment_vader(df):\n",
        "    def analyze_sentiment_vader(text):\n",
        "        scores = analyzer.polarity_scores(text)\n",
        "        compound_score = scores['compound']\n",
        "        return compound_score\n",
        "    # returns value from -1 to 1 in a new df column\n",
        "    df['tweet-sentiment-vader'] = df[\"Tweet\"].apply(analyze_sentiment_vader)\n",
        "    return df\n",
        "\n",
        "americanexpress_df = analyze_sentiment_vader(americanexpress_df)\n",
        "bankofamerica_df = analyze_sentiment_vader(bankofamerica_df)\n",
        "wellsfargo_df = analyze_sentiment_vader(wellsfargo_df)\n",
        "capitalone_df = analyze_sentiment_vader(capitalone_df)\n",
        "chase_df = analyze_sentiment_vader(chase_df)\n",
        "citi_df = analyze_sentiment_vader(citi_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J2k4yVS2kO1"
      },
      "source": [
        "Creates a scatterplot of Vader sentiment analysis results for each bank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oqiuu0fmSjkZ"
      },
      "outputs": [],
      "source": [
        "bank_dfs = {\n",
        "    \"Citi\": citi_df,\n",
        "    \"American Express\": americanexpress_df,\n",
        "    \"Bank of America\": bankofamerica_df,\n",
        "    \"Wells Fargo\": wellsfargo_df,\n",
        "    \"Capital One\": capitalone_df,\n",
        "    \"Chase\": chase_df\n",
        "}\n",
        "\n",
        "for bank_name, df in bank_dfs.items():\n",
        "    # change plot opacity depending on number of tweets\n",
        "    rows_num = df.shape[0]\n",
        "    alpha = 1200 / rows_num\n",
        "\n",
        "    # make timestamp format understandable\n",
        "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], format=\"%b %d, %Y · %I:%M %p UTC\")\n",
        "\n",
        "    # scatterplot every tweet (timestamp against score)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(df[\"Timestamp\"], df[\"tweet-sentiment-vader\"], c='b', alpha=alpha)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Sentiment Score\")\n",
        "    plt.title(\"Sentiment Score vs. Time for \" + bank_name)\n",
        "    plt.grid(True)\n",
        "\n",
        "    # set consistent limmits\n",
        "    xlim_start = pd.to_datetime(\"2014-01-01\")\n",
        "    xlim_end = pd.to_datetime(\"2023-08-04\")\n",
        "    plt.xlim([xlim_start, xlim_end])\n",
        "\n",
        "    # extract x, y values\n",
        "    x = df[\"Timestamp\"].values.astype(np.int64) // 10 ** 9\n",
        "    y = df[\"tweet-sentiment-vader\"].values\n",
        "\n",
        "    # create trend line\n",
        "    m, b = np.polyfit(x, y, 1)\n",
        "    plt.plot(df[\"Timestamp\"], m*x + b, color='red')\n",
        "\n",
        "    x_range = np.max(x) - np.min(x)\n",
        "    y_range = np.max(y) - np.min(y)\n",
        "    normalized_slope = m / (y_range / x_range)\n",
        "\n",
        "    # display slope * 100\n",
        "    slope_text = f\"Slope: {normalized_slope * 100}\"\n",
        "    plt.text(df[\"Timestamp\"].iloc[0], m*x[0] + b, slope_text[:12], fontsize=14, color='red', ha='right', va='bottom')\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}